{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T_dif(100)v1-0.001",
      "provenance": [],
      "authorship_tag": "ABX9TyNOwQBrgepRxuxEj2iXWIKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vita8759/4_Layers/blob/main/T_dif(100)v1_0_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VoD1fdbYOqN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzebVN_SbvBJ",
        "outputId": "4aa7dcb9-0bb7-4d22-acd8-41853ebd1aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# kaggle api\n",
        "api_token = {\"username\":\"pinhsuanlee\",\"key\":\"28c330210349e9af127aaf6f88dfa39f\"}\n",
        " \n",
        "if not os.path.exists(\"/root/.kaggle\"):\n",
        "    os.makedirs(\"/root/.kaggle\")\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        " \n",
        "if not os.path.exists(\"/kaggle\"):\n",
        "    os.makedirs(\"/kaggle\")\n",
        "os.chdir('/kaggle')\n",
        "!kaggle datasets download -d pinhsuanlee/t-difv1 --force\n",
        " \n",
        "!ls /kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Ghojyue2Ux",
        "outputId": "b062e29a-2442-47bc-d1e6-3d0c27ad9762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading t-difv1.zip to /kaggle\n",
            "\r  0% 0.00/12.2M [00:00<?, ?B/s]\r 74% 9.00M/12.2M [00:00<00:00, 79.9MB/s]\n",
            "\r100% 12.2M/12.2M [00:00<00:00, 99.3MB/s]\n",
            "t-difv1.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip t-difv1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPhYQf-0e9oZ",
        "outputId": "6fa08787-b157-4afa-c91b-f4b6e8890ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  t-difv1.zip\n",
            "  inflating: dataset/test_set/bad/bad(33).png  \n",
            "  inflating: dataset/test_set/bad/bad(34).png  \n",
            "  inflating: dataset/test_set/bad/bad(35).png  \n",
            "  inflating: dataset/test_set/bad/bad(36).png  \n",
            "  inflating: dataset/test_set/bad/bad(37).png  \n",
            "  inflating: dataset/test_set/bad/bad(38).png  \n",
            "  inflating: dataset/test_set/bad/bad(39).png  \n",
            "  inflating: dataset/test_set/bad/bad(40).png  \n",
            "  inflating: dataset/test_set/good/good(11).png  \n",
            "  inflating: dataset/test_set/good/good(12).png  \n",
            "  inflating: dataset/training_set/bad/bad(1).png  \n",
            "  inflating: dataset/training_set/bad/bad(10).png  \n",
            "  inflating: dataset/training_set/bad/bad(11).png  \n",
            "  inflating: dataset/training_set/bad/bad(12).png  \n",
            "  inflating: dataset/training_set/bad/bad(13).png  \n",
            "  inflating: dataset/training_set/bad/bad(14).png  \n",
            "  inflating: dataset/training_set/bad/bad(15).png  \n",
            "  inflating: dataset/training_set/bad/bad(16).png  \n",
            "  inflating: dataset/training_set/bad/bad(17).png  \n",
            "  inflating: dataset/training_set/bad/bad(18).png  \n",
            "  inflating: dataset/training_set/bad/bad(19).png  \n",
            "  inflating: dataset/training_set/bad/bad(2).png  \n",
            "  inflating: dataset/training_set/bad/bad(20).png  \n",
            "  inflating: dataset/training_set/bad/bad(21).png  \n",
            "  inflating: dataset/training_set/bad/bad(22).png  \n",
            "  inflating: dataset/training_set/bad/bad(23).png  \n",
            "  inflating: dataset/training_set/bad/bad(24).png  \n",
            "  inflating: dataset/training_set/bad/bad(25).png  \n",
            "  inflating: dataset/training_set/bad/bad(26).png  \n",
            "  inflating: dataset/training_set/bad/bad(27).png  \n",
            "  inflating: dataset/training_set/bad/bad(28).png  \n",
            "  inflating: dataset/training_set/bad/bad(29).png  \n",
            "  inflating: dataset/training_set/bad/bad(3).png  \n",
            "  inflating: dataset/training_set/bad/bad(30).png  \n",
            "  inflating: dataset/training_set/bad/bad(31).png  \n",
            "  inflating: dataset/training_set/bad/bad(32).png  \n",
            "  inflating: dataset/training_set/bad/bad(4).png  \n",
            "  inflating: dataset/training_set/bad/bad(5).png  \n",
            "  inflating: dataset/training_set/bad/bad(6).png  \n",
            "  inflating: dataset/training_set/bad/bad(7).png  \n",
            "  inflating: dataset/training_set/bad/bad(8).png  \n",
            "  inflating: dataset/training_set/bad/bad(9).png  \n",
            "  inflating: dataset/training_set/good/good(1).png  \n",
            "  inflating: dataset/training_set/good/good(10).png  \n",
            "  inflating: dataset/training_set/good/good(2).png  \n",
            "  inflating: dataset/training_set/good/good(3).png  \n",
            "  inflating: dataset/training_set/good/good(4).png  \n",
            "  inflating: dataset/training_set/good/good(5).png  \n",
            "  inflating: dataset/training_set/good/good(6).png  \n",
            "  inflating: dataset/training_set/good/good(7).png  \n",
            "  inflating: dataset/training_set/good/good(8).png  \n",
            "  inflating: dataset/training_set/good/good(9).png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the neural net class\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20, \n",
        "                               kernel_size=13, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=30, \n",
        "                               kernel_size=13, stride=4)\n",
        "        self.fc1 = nn.Linear(in_features=2430, out_features=1000)\n",
        "        self.fc2 = nn.Linear(in_features=1000, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "empclS_ve_jh",
        "outputId": "eb053da6-58c7-42c2-9478-6b7c1e1f8fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 20, kernel_size=(13, 13), stride=(4, 4))\n",
            "  (conv2): Conv2d(20, 30, kernel_size=(13, 13), stride=(4, 4))\n",
            "  (fc1): Linear(in_features=2430, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "batch_size = 1\n",
        "num_epochs = 5\n",
        "lr = 0.001\n",
        "num_classes=2"
      ],
      "metadata": {
        "id": "vjGQz-3tfF-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform\n",
        "transform = transforms.Compose(\n",
        "                [transforms.Resize(size=(700,700)),\n",
        "                 transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),]\n",
        "                )\n",
        "\n",
        "# Data\n",
        "train_dataset = datasets.ImageFolder(root='/kaggle/dataset/training_set', transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(root='/kaggle/dataset/test_set', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fqX2fb05fJOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "def train(net, train_loader):\n",
        "\n",
        "    for epoch in range(100): # no. of epochs\n",
        "        running_loss_1 = 0\n",
        "        for data in train_loader:\n",
        "            # data pixels and labels to GPU if available\n",
        "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
        "            # set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # propagate the loss backward\n",
        "            loss.backward()\n",
        "            # update the gradients\n",
        "            optimizer.step()\n",
        " \n",
        "            running_loss_1= loss.item()\n",
        "        print('[Epoch %d] train_loss: %.3f' %\n",
        "                      (epoch + 1, running_loss_1/len(train_loader)))\n",
        "\n",
        "    running_loss_1 = []\n",
        "    epoch = []        \n",
        "\n",
        "\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            print(predicted)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on train images: %0.4f %%' % (\n",
        "        100 * train_correct / train_total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "def test(net, valid_loader):\n",
        "      for epoch in range(100): # no. of epochs\n",
        "        running_loss_2 = 0\n",
        "        for data in train_loader:\n",
        "            # data pixels and labels to GPU if available\n",
        "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
        "            # set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # propagate the loss backward\n",
        "            loss.backward()\n",
        "            # update the gradients\n",
        "            optimizer.step()\n",
        " \n",
        "            running_loss_2= loss.item()\n",
        "        print('[Epoch %d] test_loss: %.3f' %\n",
        "                      (epoch + 1, running_loss_2/len(valid_loader)))\n",
        "        \n",
        "      running_loss_2 = []\n",
        "      epoch = [] \n",
        "        \n",
        "\n",
        "\n",
        "      test_correct = 0\n",
        "      test_total = 0\n",
        "      with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            print(predicted)\n",
        "\n",
        "\n",
        "      print('Accuracy of the network on test images: %0.4f %%' % (\n",
        "        100 * test_correct / test_total))\n",
        "       \n",
        " \n",
        "\n",
        "\n",
        "      running_loss = []\n",
        "      epoch = [] \n",
        "\n",
        "    \n",
        "\n",
        "train(net, train_loader)\n",
        "test(net, valid_loader)       \n",
        "\n",
        "end = time.time()\n",
        "print('Import Time :', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cnCKGDyjB9d",
        "outputId": "7b273289-6ed3-48cb-eaea-2e27439b677f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss: 0.001\n",
            "[Epoch 2] train_loss: 0.047\n",
            "[Epoch 3] train_loss: 0.038\n",
            "[Epoch 4] train_loss: 0.002\n",
            "[Epoch 5] train_loss: 0.014\n",
            "[Epoch 6] train_loss: 0.023\n",
            "[Epoch 7] train_loss: 0.010\n",
            "[Epoch 8] train_loss: 0.003\n",
            "[Epoch 9] train_loss: 0.028\n",
            "[Epoch 10] train_loss: 0.004\n",
            "[Epoch 11] train_loss: 0.012\n",
            "[Epoch 12] train_loss: 0.006\n",
            "[Epoch 13] train_loss: 0.005\n",
            "[Epoch 14] train_loss: 0.010\n",
            "[Epoch 15] train_loss: 0.035\n",
            "[Epoch 16] train_loss: 0.055\n",
            "[Epoch 17] train_loss: 0.009\n",
            "[Epoch 18] train_loss: 0.008\n",
            "[Epoch 19] train_loss: 0.006\n",
            "[Epoch 20] train_loss: 0.008\n",
            "[Epoch 21] train_loss: 0.036\n",
            "[Epoch 22] train_loss: 0.004\n",
            "[Epoch 23] train_loss: 0.007\n",
            "[Epoch 24] train_loss: 0.050\n",
            "[Epoch 25] train_loss: 0.006\n",
            "[Epoch 26] train_loss: 0.005\n",
            "[Epoch 27] train_loss: 0.008\n",
            "[Epoch 28] train_loss: 0.008\n",
            "[Epoch 29] train_loss: 0.007\n",
            "[Epoch 30] train_loss: 0.007\n",
            "[Epoch 31] train_loss: 0.008\n",
            "[Epoch 32] train_loss: 0.005\n",
            "[Epoch 33] train_loss: 0.009\n",
            "[Epoch 34] train_loss: 0.008\n",
            "[Epoch 35] train_loss: 0.008\n",
            "[Epoch 36] train_loss: 0.008\n",
            "[Epoch 37] train_loss: 0.005\n",
            "[Epoch 38] train_loss: 0.007\n",
            "[Epoch 39] train_loss: 0.011\n",
            "[Epoch 40] train_loss: 0.007\n",
            "[Epoch 41] train_loss: 0.010\n",
            "[Epoch 42] train_loss: 0.008\n",
            "[Epoch 43] train_loss: 0.026\n",
            "[Epoch 44] train_loss: 0.009\n",
            "[Epoch 45] train_loss: 0.005\n",
            "[Epoch 46] train_loss: 0.020\n",
            "[Epoch 47] train_loss: 0.005\n",
            "[Epoch 48] train_loss: 0.002\n",
            "[Epoch 49] train_loss: 0.003\n",
            "[Epoch 50] train_loss: 0.005\n",
            "[Epoch 51] train_loss: 0.003\n",
            "[Epoch 52] train_loss: 0.002\n",
            "[Epoch 53] train_loss: 0.000\n",
            "[Epoch 54] train_loss: 0.000\n",
            "[Epoch 55] train_loss: 0.000\n",
            "[Epoch 56] train_loss: 0.000\n",
            "[Epoch 57] train_loss: 0.000\n",
            "[Epoch 58] train_loss: 0.000\n",
            "[Epoch 59] train_loss: 0.000\n",
            "[Epoch 60] train_loss: 0.000\n",
            "[Epoch 61] train_loss: 0.000\n",
            "[Epoch 62] train_loss: 0.000\n",
            "[Epoch 63] train_loss: 0.000\n",
            "[Epoch 64] train_loss: 0.000\n",
            "[Epoch 65] train_loss: 0.001\n",
            "[Epoch 66] train_loss: 0.000\n",
            "[Epoch 67] train_loss: 0.000\n",
            "[Epoch 68] train_loss: 0.000\n",
            "[Epoch 69] train_loss: 0.000\n",
            "[Epoch 70] train_loss: 0.000\n",
            "[Epoch 71] train_loss: 0.000\n",
            "[Epoch 72] train_loss: 0.001\n",
            "[Epoch 73] train_loss: 0.000\n",
            "[Epoch 74] train_loss: 0.000\n",
            "[Epoch 75] train_loss: 0.000\n",
            "[Epoch 76] train_loss: 0.000\n",
            "[Epoch 77] train_loss: 0.000\n",
            "[Epoch 78] train_loss: 0.000\n",
            "[Epoch 79] train_loss: 0.000\n",
            "[Epoch 80] train_loss: 0.000\n",
            "[Epoch 81] train_loss: 0.000\n",
            "[Epoch 82] train_loss: 0.000\n",
            "[Epoch 83] train_loss: 0.000\n",
            "[Epoch 84] train_loss: 0.000\n",
            "[Epoch 85] train_loss: 0.000\n",
            "[Epoch 86] train_loss: 0.000\n",
            "[Epoch 87] train_loss: 0.000\n",
            "[Epoch 88] train_loss: 0.000\n",
            "[Epoch 89] train_loss: 0.000\n",
            "[Epoch 90] train_loss: 0.000\n",
            "[Epoch 91] train_loss: 0.000\n",
            "[Epoch 92] train_loss: 0.000\n",
            "[Epoch 93] train_loss: 0.000\n",
            "[Epoch 94] train_loss: 0.000\n",
            "[Epoch 95] train_loss: 0.000\n",
            "[Epoch 96] train_loss: 0.000\n",
            "[Epoch 97] train_loss: 0.000\n",
            "[Epoch 98] train_loss: 0.000\n",
            "[Epoch 99] train_loss: 0.000\n",
            "[Epoch 100] train_loss: 0.000\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([1])\n",
            "Accuracy of the network on train images: 100.0000 %\n",
            "[Epoch 1] test_loss: 0.000\n",
            "[Epoch 2] test_loss: 0.000\n",
            "[Epoch 3] test_loss: 0.000\n",
            "[Epoch 4] test_loss: 0.000\n",
            "[Epoch 5] test_loss: 0.000\n",
            "[Epoch 6] test_loss: 0.000\n",
            "[Epoch 7] test_loss: 0.000\n",
            "[Epoch 8] test_loss: 0.001\n",
            "[Epoch 9] test_loss: 0.000\n",
            "[Epoch 10] test_loss: 0.000\n",
            "[Epoch 11] test_loss: 0.000\n",
            "[Epoch 12] test_loss: 0.001\n",
            "[Epoch 13] test_loss: 0.000\n",
            "[Epoch 14] test_loss: 0.000\n",
            "[Epoch 15] test_loss: 0.000\n",
            "[Epoch 16] test_loss: 0.000\n",
            "[Epoch 17] test_loss: 0.000\n",
            "[Epoch 18] test_loss: 0.000\n",
            "[Epoch 19] test_loss: 0.000\n",
            "[Epoch 20] test_loss: 0.000\n",
            "[Epoch 21] test_loss: 0.000\n",
            "[Epoch 22] test_loss: 0.000\n",
            "[Epoch 23] test_loss: 0.000\n",
            "[Epoch 24] test_loss: 0.000\n",
            "[Epoch 25] test_loss: 0.000\n",
            "[Epoch 26] test_loss: 0.000\n",
            "[Epoch 27] test_loss: 0.000\n",
            "[Epoch 28] test_loss: 0.000\n",
            "[Epoch 29] test_loss: 0.000\n",
            "[Epoch 30] test_loss: 0.000\n",
            "[Epoch 31] test_loss: 0.000\n",
            "[Epoch 32] test_loss: 0.000\n",
            "[Epoch 33] test_loss: 0.000\n",
            "[Epoch 34] test_loss: 0.000\n",
            "[Epoch 35] test_loss: 0.000\n",
            "[Epoch 36] test_loss: 0.000\n",
            "[Epoch 37] test_loss: 0.000\n",
            "[Epoch 38] test_loss: 0.000\n",
            "[Epoch 39] test_loss: 0.000\n",
            "[Epoch 40] test_loss: 0.000\n",
            "[Epoch 41] test_loss: 0.000\n",
            "[Epoch 42] test_loss: 0.000\n",
            "[Epoch 43] test_loss: 0.000\n",
            "[Epoch 44] test_loss: 0.000\n",
            "[Epoch 45] test_loss: 0.000\n",
            "[Epoch 46] test_loss: 0.000\n",
            "[Epoch 47] test_loss: 0.000\n",
            "[Epoch 48] test_loss: 0.000\n",
            "[Epoch 49] test_loss: 0.000\n",
            "[Epoch 50] test_loss: 0.000\n",
            "[Epoch 51] test_loss: 0.000\n",
            "[Epoch 52] test_loss: 0.000\n",
            "[Epoch 53] test_loss: 0.000\n",
            "[Epoch 54] test_loss: 0.000\n",
            "[Epoch 55] test_loss: 0.000\n",
            "[Epoch 56] test_loss: 0.000\n",
            "[Epoch 57] test_loss: 0.000\n",
            "[Epoch 58] test_loss: 0.000\n",
            "[Epoch 59] test_loss: 0.000\n",
            "[Epoch 60] test_loss: 0.000\n",
            "[Epoch 61] test_loss: 0.000\n",
            "[Epoch 62] test_loss: 0.000\n",
            "[Epoch 63] test_loss: 0.000\n",
            "[Epoch 64] test_loss: 0.000\n",
            "[Epoch 65] test_loss: 0.000\n",
            "[Epoch 66] test_loss: 0.000\n",
            "[Epoch 67] test_loss: 0.000\n",
            "[Epoch 68] test_loss: 0.000\n",
            "[Epoch 69] test_loss: 0.000\n",
            "[Epoch 70] test_loss: 0.000\n",
            "[Epoch 71] test_loss: 0.000\n",
            "[Epoch 72] test_loss: 0.000\n",
            "[Epoch 73] test_loss: 0.000\n",
            "[Epoch 74] test_loss: 0.000\n",
            "[Epoch 75] test_loss: 0.000\n",
            "[Epoch 76] test_loss: 0.000\n",
            "[Epoch 77] test_loss: 0.000\n",
            "[Epoch 78] test_loss: 0.000\n",
            "[Epoch 79] test_loss: 0.000\n",
            "[Epoch 80] test_loss: 0.000\n",
            "[Epoch 81] test_loss: 0.000\n",
            "[Epoch 82] test_loss: 0.000\n",
            "[Epoch 83] test_loss: 0.000\n",
            "[Epoch 84] test_loss: 0.000\n",
            "[Epoch 85] test_loss: 0.000\n",
            "[Epoch 86] test_loss: 0.000\n",
            "[Epoch 87] test_loss: 0.000\n",
            "[Epoch 88] test_loss: 0.000\n",
            "[Epoch 89] test_loss: 0.000\n",
            "[Epoch 90] test_loss: 0.000\n",
            "[Epoch 91] test_loss: 0.000\n",
            "[Epoch 92] test_loss: 0.000\n",
            "[Epoch 93] test_loss: 0.000\n",
            "[Epoch 94] test_loss: 0.000\n",
            "[Epoch 95] test_loss: 0.000\n",
            "[Epoch 96] test_loss: 0.000\n",
            "[Epoch 97] test_loss: 0.000\n",
            "[Epoch 98] test_loss: 0.000\n",
            "[Epoch 99] test_loss: 0.000\n",
            "[Epoch 100] test_loss: 0.000\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "tensor([0])\n",
            "Accuracy of the network on test images: 80.0000 %\n",
            "Import Time : 746.7030355930328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ciRYuS_Yu6x3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}